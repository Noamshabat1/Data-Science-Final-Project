#!/usr/bin/env python
# ──────────────────────────────────────────────────────────────────────────────
# FinalProject_ML_pipeline_v3.py
# ──────────────────────────────────────────────────────────────────────────────
from __future__ import annotations
import argparse, logging, sys, warnings
from pathlib import Path
from typing import Dict, List, Any

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    mean_squared_error, r2_score, precision_recall_curve,
    accuracy_score, roc_auc_score, ConfusionMatrixDisplay,
    RocCurveDisplay, PrecisionRecallDisplay
)
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.stats import uniform, randint
import xgboost as xgb
from xgboost import XGBClassifier

try:
    import shap
    SHAP_OK = True
except ModuleNotFoundError:
    SHAP_OK = False

# ─────────────────── paths / config ──────────────────────────────────────────
ROOT        = Path(__file__).resolve().parents[1]
DATA_CLEAN  = ROOT / "Data" / "clean"
DATA_ORIG   = ROOT / "Data" / "original"
MODEL_OUT   = ROOT / "Model" / "xgb_tsla.json"

US_TZ       = "America/New_York"
MKT_CLOSE   = pd.Timestamp("16:00").time()                        # 4 p.m. Eastern
_PARSE_FMT  = "%Y-%m-%d %H:%M:%S%z"                               # exact tweet fmt

CONFIG: Dict[str, Any] = {
    "RANDOM_STATE": 42,
    "MAX_JOBS": 8,
    "STOCK_DATE_COL": "Price",            # ← exact header in your CSV
    "TWEET_FILES": [
        "clean_musk_tweets.csv",
        "clean_musk_retweets.csv",
        "clean_musk_replies.csv",
    ],
    "STOCK_FILE": "tesla_stock_data_2000_2025.csv",
    "KEYWORDS": [
        "model 3", "model y", "cybertruck", "ai",
        "robot", "teslabot", "fsd", "tesla energy", "spacex",
    ],
    "TFIDF_MAX": 400,
    "BASE_XGB": dict(
        n_estimators=800,
        objective="binary:logistic",
        eval_metric="logloss",
        tree_method="hist",
    ),
    "HPARAM_SPACE": {
        "learning_rate":    uniform(0.01, 0.19),
        "max_depth":        randint(3, 9),
        "subsample":        uniform(0.6, 0.4),
        "colsample_bytree": uniform(0.6, 0.4),
        "gamma":            uniform(0.0, 4.0),
        "reg_lambda":       uniform(0.5, 4.5),
    },
}

# ─────────────────── logging helper ──────────────────────────────────────────
logging.basicConfig(level=logging.INFO,
    format="%(asctime)s [%(levelname)6s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)])
log = logging.getLogger(__name__)
def banner(msg:str)->None:
    log.info("="*100); log.info(msg); log.info("="*100)
def _req(p:Path)->None:
    if not p.exists(): raise FileNotFoundError(p)

# ─────────────────── Tweet utilities ─────────────────────────────────────────
def _finbert_sentiment(texts:pd.Series)->pd.Series:
    try:
        from transformers import pipeline
        pipe = pipeline("sentiment-analysis",
                        model="ProsusAI/finbert",
                        tokenizer="ProsusAI/finbert",
                        device=-1)
        out   = pipe(texts.tolist(), truncation=True, batch_size=32)
        score = {"positive": 1, "neutral": 0, "negative": -1}
        return pd.Series([score[o["label"].lower()] * o["score"] for o in out],
                         index=texts.index)
    except Exception as e:
        log.warning("FinBERT unavailable (%s) – neutral sentiment applied.", e)
        return pd.Series(np.zeros(len(texts)), index=texts.index)

def _std_post(df:pd.DataFrame, src:str)->pd.DataFrame:
    txt = ("cleanedReplyContent" if "cleanedReplyContent" in df.columns
           else "fullText" if "fullText" in df.columns
           else "originalContent")
    ts_col = "createdAt" if "createdAt" in df.columns else "created_at"
    dt_utc = pd.to_datetime(df[ts_col], utc=True, errors="coerce", format=_PARSE_FMT)
    dt_loc = dt_utc.dt.tz_convert(US_TZ)

    mkt_date = dt_loc.dt.date.copy()
    after_close = dt_loc.dt.time > MKT_CLOSE
    mkt_date[after_close] = (dt_loc[after_close] + pd.Timedelta(days=1)).dt.date

    return pd.DataFrame({
        "mkt_date":  mkt_date,
        "text":      df[txt].fillna(""),
        "sentiment": df.get("sentiment", 0.0),
        "source":    src,
    })

def load_posts(finbert:bool=False)->pd.DataFrame:
    frames: List[pd.DataFrame] = []
    for fn in CONFIG["TWEET_FILES"]:
        p = DATA_CLEAN / fn; _req(p)
        src = fn.split("_")[2].split(".")[0]
        frames.append(_std_post(pd.read_csv(p), src))
    posts = pd.concat(frames, ignore_index=True)
    if finbert:
        log.info("Running FinBERT sentiment …")
        posts["sentiment"] = _finbert_sentiment(posts["text"])
    return posts

# ─────────────────── Tweet aggregation (daily) ───────────────────
def agg_daily(posts: pd.DataFrame) -> pd.DataFrame:
    for kw in CONFIG["KEYWORDS"]:
        col = f"kw_{kw.replace(' ', '_')}"
        posts[col] = posts["text"].str.contains(
            kw, case=False, regex=False, na=False).astype(int)

    agg = {
        "text_cat":      ("text", " ".join),
        "tweet_count":   ("text", "size"),
        "sentiment_mean":("sentiment", "mean"),
    } | {
        f"kw_{k.replace(' ', '_')}": (f"kw_{k.replace(' ', '_')}", "sum")
        for k in CONFIG["KEYWORDS"]
    }

    out = posts.groupby("mkt_date").agg(**agg).reset_index()
    out["mkt_date"] = pd.to_datetime(out["mkt_date"]).dt.date
    return out

# ─────────────────── helper to detect date column ─────────────────
def _detect_date(raw: pd.DataFrame, hints: list[str]) -> str:
    override = CONFIG.get("STOCK_DATE_COL")
    if override and override in raw.columns:
        return override

    hint_set = {h.lower() for h in hints}
    for c in raw.columns:
        if c.lower() in hint_set:
            return c

    for col in raw.columns:
        s = pd.to_datetime(raw[col].head(20), errors="coerce")
        if s.notna().sum() >= 0.8 * len(s):
            return col

    raise ValueError("No suitable date column found in stock CSV.")

# ─────────────────── Market loader ────────────────────────────────
def load_market() -> pd.DataFrame:
    p = DATA_ORIG / CONFIG["STOCK_FILE"]; _req(p)

    raw = pd.read_csv(p, skiprows=[1])

    dt_col = _detect_date(raw, [
        "datetime","timestamp","date","Datetime","Date","DateTime","DATE"
    ])                                             # returns "Price" via override

    # close-price column via simple header match
    close_candidates = ["Close", "Adj Close", "close", "close_price"]
    close_col = next((c for c in close_candidates if c in raw.columns), None)
    if close_col is None:
        raise ValueError("No close-price column found in stock CSV.")

    raw[dt_col]   = pd.to_datetime(raw[dt_col], errors="coerce").dt.date
    raw[close_col] = pd.to_numeric(raw[close_col], errors="coerce")

    close = (
        raw[[dt_col, close_col]]
        .dropna()
        .groupby(dt_col)[close_col]
        .last()
    )

    df = pd.DataFrame({
        "close":       close,
        "ret_fwd_1d":  close.pct_change().shift(-1),
        "ret_lag_1d":  close.pct_change(),
        "vol_5d":      close.pct_change().rolling(5).std(),
    }).dropna()
    df.index.name = "mkt_date"
    return df

def merge_daily(tweets:pd.DataFrame, market:pd.DataFrame)->pd.DataFrame:
    return pd.merge(tweets, market, left_on="mkt_date", right_index=True,
                    how="inner").sort_values("mkt_date")

# ─────────────────── TF-IDF helper ───────────────────────────────
class TrainOnlyTFIDF:
    def __init__(self, *, max_feats:int, keep:int):
        self.vec  = TfidfVectorizer(max_features=max_feats,
                                    ngram_range=(1,2), min_df=3)
        self.keep = keep
    def fit(self, text_train:pd.Series, y_train:pd.Series):
        tf = self.vec.fit_transform(text_train)
        corr = np.array([np.corrcoef(tf[:,i].toarray().ravel(), y_train)[0,1]
                         for i in range(tf.shape[1])])
        self.sel_ = np.argsort(np.nan_to_num(np.abs(corr)))[-self.keep:]
    def transform(self, full_text:pd.Series)->pd.DataFrame:
        tf = self.vec.transform(full_text)[:, self.sel_]
        cols = [f"tfidf_{t}"
                for t in self.vec.get_feature_names_out()[self.sel_]]
        return pd.DataFrame(tf.toarray(), columns=cols, index=full_text.index)

# ─────────────────── Ridge baseline ──────────────────────────────
def ridge_baseline(df:pd.DataFrame, split:float)->None:
    banner("📊  Ridge baseline (numeric return)")
    kw = [f"kw_{k.replace(' ','_')}" for k in CONFIG["KEYWORDS"]]
    X  = df[["sentiment_mean","tweet_count","ret_lag_1d","vol_5d",*kw]].to_numpy()
    y  = df["ret_fwd_1d"].to_numpy()
    idx = int(split * len(df)); Xtr,Xte = X[:idx], X[idx:]; ytr,yte = y[:idx], y[idx:]
    sc  = StandardScaler().fit(Xtr); Xtr_s,Xte_s = sc.transform(Xtr), sc.transform(Xte)
    gcv = GridSearchCV(
        Ridge(random_state=CONFIG["RANDOM_STATE"]),
        {"alpha":[10**i for i in range(-4,3)]},
        cv=TimeSeriesSplit(5),
        scoring="neg_root_mean_squared_error"
    ).fit(Xtr_s,ytr)
    best = gcv.best_params_["alpha"]
    preds = gcv.predict(Xte_s)
    log.info("α=%g  RMSE=%.5f  R²=%.4f",
             best, np.sqrt(mean_squared_error(yte,preds)), r2_score(yte,preds))
    plt.figure(figsize=(4,4))
    plt.scatter(yte, preds, alpha=.5)
    plt.axline((0,0), slope=1, linestyle="--", color="k")
    plt.title(f"Ridge α={best}"); plt.tight_layout(); plt.show()

# ─────────────────── XGB model ───────────────────────────────────
def xgb_model(df:pd.DataFrame, *, split:float, tfidf_max:int, n_iter:int)->None:
    banner("🚀  XGBoost (direction)")
    idx_test = int(split * len(df)); idx_val = int(idx_test * 0.9)

    y_full = (df["ret_fwd_1d"] > 0).astype(int)

    banner(f"📝  TF-IDF selection  (max={tfidf_max})")
    tfidf = TrainOnlyTFIDF(max_feats=6000, keep=tfidf_max)
    tfidf.fit(df["text_cat"].iloc[:idx_val], y_full.iloc[:idx_val])
    tf_df  = tfidf.transform(df["text_cat"])

    base = ["sentiment_mean","tweet_count","ret_lag_1d","vol_5d"] + \
           [f"kw_{k.replace(' ','_')}" for k in CONFIG["KEYWORDS"]]
    X_full = pd.concat([df[base], tf_df], axis=1).astype("float32")

    X_train, y_train = X_full.iloc[:idx_val],      y_full.iloc[:idx_val]
    X_val,   y_val   = X_full.iloc[idx_val:idx_test], y_full.iloc[idx_val:idx_test]
    X_test,  y_test  = X_full.iloc[idx_test:],     y_full.iloc[idx_test:]

    base_params = dict(CONFIG["BASE_XGB"],
        random_state=CONFIG["RANDOM_STATE"],
        n_jobs=CONFIG["MAX_JOBS"],
        scale_pos_weight=(y_train==0).sum()/max((y_train==1).sum(),1)
    )

    search = RandomizedSearchCV(
        XGBClassifier(**base_params),
        CONFIG["HPARAM_SPACE"], n_iter=n_iter,
        cv=TimeSeriesSplit(4), scoring="roc_auc",
        n_jobs=CONFIG["MAX_JOBS"],
        random_state=CONFIG["RANDOM_STATE"], verbose=0
    ).fit(X_train, y_train)

    best_params = search.best_params_
    log.info("Best params: %s", best_params)

    model = XGBClassifier(**base_params | best_params)

    # ── robust early-stopping across xgboost versions ─────────────
    fitted = False
    try:
        model.fit(X_train, y_train,
                  eval_set=[(X_train, y_train), (X_val, y_val)],
                  callbacks=[xgb.callback.EarlyStopping(rounds=50)],
                  verbose=False)
        fitted = True
    except TypeError:
        pass
    if not fitted:
        try:
            model.fit(X_train, y_train,
                      eval_set=[(X_train, y_train), (X_val, y_val)],
                      early_stopping_rounds=50,
                      verbose=False)
            fitted = True
        except TypeError:
            pass
    if not fitted:
        model = XGBClassifier(**base_params | best_params, early_stopping_round=50)
        model.fit(X_train, y_train,
                  eval_set=[(X_train, y_train), (X_val, y_val)],
                  verbose=False)

    # ── diagnostics ──────────────────────────────────────────────
    evals = model.evals_result()
    plt.figure(figsize=(6,3))
    plt.plot(evals["validation_0"]["logloss"], label="train")
    plt.plot(evals["validation_1"]["logloss"], label="val")
    plt.legend(); plt.ylabel("log-loss"); plt.xlabel("round")
    plt.title("XGB log-loss"); plt.tight_layout(); plt.show()

    prob = model.predict_proba(X_test)[:,1]
    p,r,t = precision_recall_curve(y_test, prob)
    f1 = 2*p*r/(p+r+1e-12)
    tau = t[int(f1.argmax())]; y_hat = (prob >= tau).astype(int)

    log.info("τ≈%.3f  F1=%.3f  Acc=%.3f  AUC=%.3f",
             tau, f1.max(), accuracy_score(y_test,y_hat),
             roc_auc_score(y_test,prob))

    RocCurveDisplay.from_predictions(y_test, prob); plt.title("ROC — XGB"); plt.tight_layout(); plt.show()
    PrecisionRecallDisplay.from_predictions(y_test, prob); plt.title("PR Curve — XGB"); plt.tight_layout(); plt.show()
    ConfusionMatrixDisplay.from_predictions(y_test, y_hat, cmap="viridis", colorbar=False); plt.title("Confusion Matrix — XGB"); plt.tight_layout(); plt.show()

    if SHAP_OK:
        explainer = shap.TreeExplainer(model)
        shap_vals = explainer.shap_values(X_test, check_additivity=False)
        mean_abs  = np.abs(shap_vals).mean(axis=0)
        k = 20
        idx = np.argsort(mean_abs)[-k:][::-1]
        plt.figure(figsize=(6,5))
        plt.barh(range(k), mean_abs[idx][::-1])
        plt.yticks(range(k), X_test.columns[idx][::-1])
        plt.xlabel("mean |SHAP|")
        plt.title("Top 20 features (impact)")
        plt.tight_layout(); plt.show()
    else:
        log.warning("SHAP not installed – skipping SHAP plot.")

    MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)
    model.save_model(MODEL_OUT)
    assert MODEL_OUT.is_file() and MODEL_OUT.stat().st_size > 0, "Model file missing or empty!"
    log.info("✅  Model saved → %s", MODEL_OUT)

# ─────────────────── main ─────────────────────────────────────────
def main()->None:
    warnings.filterwarnings("ignore", category=FutureWarning)
    ap = argparse.ArgumentParser()
    ap.add_argument("--split",       type=float, default=.7)
    ap.add_argument("--tfidf-max",   type=int,   default=CONFIG["TFIDF_MAX"])
    ap.add_argument("--finbert",     action="store_true")
    ap.add_argument("--search-iter", type=int,   default=40)
    args = ap.parse_args()

    banner("🏁  PIPELINE START")
    posts  = load_posts(finbert=args.finbert)
    tweets = agg_daily(posts)
    market = load_market()
    data   = merge_daily(tweets, market)
    banner(f"Merged dataset: {len(data):,} trading days")

    ridge_baseline(data, split=args.split)
    xgb_model(data, split=args.split,
              tfidf_max=args.tfidf_max,
              n_iter=args.search_iter)
    banner("✅  PIPELINE FINISHED")

if __name__ == "__main__":
    main()
