# Tesla-Tweet ML Pipeline

*Predicting next-day Tesla (TSLA) price direction from Elon Musk’s tweets.*

---

## Project Goal
We ask a single question:

> *Does Elon Musk’s Twitter activity contain information that helps
> predict whether Tesla’s stock price will go **up** or **down** the next
> trading day?*

The pipeline:

| Step | Output |
|------|--------|
| 1. Ingest tweets + market data | Daily feature table |
| 2. Baseline Ridge | RMSE / R² scatter |
| 3. Text → TF-IDF (mutual info ranking) | 400 best n-grams |
| 4. XGBoost classifier (hyper-tuned) | Saved model `Model/xgb_tsla.json` |
| 5. Diagnostics | Log-loss, ROC, PR, confusion, SHAP |

`python FinalProject_ML_pipeline_v4.py --help` shows all CLI flags.

---

## Data Sources

| Folder | File(s) | Description |
|--------|---------|-------------|
| `Data/clean/` | `clean_musk_tweets.csv`, `…_retweets.csv`, `…_replies.csv` | Pre-cleaned tweet, retweet & reply datasets. |
| `Data/original/` | `tesla_stock_data_2000_2025.csv` | Yahoo! Finance download (has two metadata rows). |
| `Model/` | *(auto-created)* | Stores the trained XGBoost booster. |

---

## Feature Engineering

### Tweet features (per *market* date)
* `tweet_count` — number of Musk posts
* `sentiment_mean` — average tweet sentiment (FinBERT optional)
* `kw_*` — counts for 9 domain-specific keywords
* `text_cat` — full concatenated text (for TF-IDF)

### Market features (per date)
| Name | Formula |
|------|---------|
| `ret_lag_1d` | \(P_t/P_{t-1}-1\) |
| `vol_5d` | 5-day stdev of pct-change |
| **Label** `ret_fwd_1d` | \(P_{t+1}/P_t -1\) → binarised to Up / Down |

**Rollover rule:** Tweets posted **after 16:00 ET** are assigned to the
*next* trading day.

---

## Modelling

| Model | Notes |
|-------|-------|
| **Ridge** | Baseline; features scaled; α via grid-search. |
| **XGBoost** | 800 trees, hist algo, class weight √(neg/pos).<br>RandomisedSearch 40 × 6 hp on 4-fold TimeSeries CV.<br>Early-stopping 50 rounds (compatible with XGB < 1.7 → 3.0). |
| **TF-IDF** | 1–2-grams, `min_df=5`, `max_df=0.8`, 6 000 raw terms → top-400 by *mutual information* with label. |

Diagnostics: train/val log-loss, ROC-AUC, PR-AUC, F1-optimised threshold,
confusion matrix, SHAP bar plot (mean \|SHAP\|).

---

## Running the Pipeline

```bash
# activate venv first …
python src/FinalProject_ML_pipeline_v4.py \
       --split 0.7           # 70 % train, 30 % test
       --tfidf-max 400       # keep 400 n-grams
       --search-iter 40      # random-search iterations
       # --finbert           # (optional) recompute tweet sentiment
```

Outputs land in `Model/` + plots pop up interactively.

---

## Assumptions & Caveats
* 1-day TSLA returns are noisy; expect modest AUC (≈0.55 after tuning).
* Sentiment defaults to neutral unless `--finbert` is used.
* No macro factors (S&P, VIX) yet.
* Static keyword list — update for new products (“Dojo”, “Optimus” …).

---

## Road-Map Ideas
* Add SPY lag & VIX to de-beta return.
* Rolling 5-fold expanding-window CV.
* Five-day forward label for smoother signal.
* Sentiment × (1/volatility) interaction term.
* Better sentiment (FinBERT + sarcasm filter).
