# -*- coding: utf-8 -*-
"""Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jeu_3YoPQFTcSs64axKfvadew0yHRYie

# Elon Musk Tweets → TSLA Intraday Impact  (v9, full code)
# =================================================

Pipeline:
--------
1. Load raw CSVs and clean headers
2. Save cleaned copies to 'clean Data/' next to raw files
3. Add VADER sentiment, text length, keyword flags
4. Align tweets to minute-bar TSLA prices + build forward returns
6. Baseline 2 — Ridge with CV          (RMSE + R²)
7. Advanced  — Gradient-boosted XGB    (full diagnostics)

##Imports & global settings
"""

import os, re, warnings
from typing import List, Tuple, Dict

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import statsmodels.api as sm
from statsmodels.regression.linear_model import OLS

from sklearn.linear_model import Ridge
from sklearn.metrics import (mean_squared_error, r2_score,
                             accuracy_score, precision_score,
                             recall_score, f1_score, roc_auc_score,
                             RocCurveDisplay, ConfusionMatrixDisplay,
                             precision_recall_curve)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV

from xgboost import XGBClassifier

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

warnings.filterwarnings("ignore")
plt.style.use("ggplot")
RANDOM_STATE = 42

"""## Paths & hyper-parameters"""

try:
    from google.colab import drive

    drive.mount("/content/drive")
except ImportError:
    pass

BASE_DIR = "/content/drive/My Drive/Notability/Year 4/Second semester/Data Science/Final Project/Data"
MUSK_CSV = f"{BASE_DIR}/all_musk_posts.csv"
TSLA_CSV = f"{BASE_DIR}/tesla_stock_data_2000_2025.csv"
CLEAN_DIR = f"{BASE_DIR}/Clean Data"
os.makedirs(CLEAN_DIR, exist_ok=True)


KEYWORDS: List[str] = [
    "model 3", "model y", "cybertruck", "ai", "robot",
    "teslabot", "fsd", "tesla energy", "spacex",
]

RET_WINDOWS: List[int] = [5, 30, 60]

XGB_PARAMS: Dict[str, object] = {
    "n_estimators": 400,
    "learning_rate": 0.03,
    "max_depth": 5,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "objective": "binary:logistic",
    "eval_metric": "logloss",
    "random_state": RANDOM_STATE,
    "n_jobs": -1,
}

# ── 2. Helper — neat console banners ───────────────────────
def banner(txt: str) -> None:
    print(f"\n{'─' * 60}\n{txt}\n{'─' * 60}")


# ── 2-B. Mini utility to show a *tidy* snapshot of any DataFrame ─────────
def snapshot_df(df: pd.DataFrame,
                label: str,
                *,
                head_n: int = 4,
                preview_n: int = 6,
                baseline: pd.DataFrame | None = None
                ) -> None:
    """
    Compact one-pager for a DataFrame.

    • headline stats (shape, memory, dtype counts, % rows with ≥1 NA)
    • if *baseline* is supplied, only columns that changed (added OR dtype-fixed)
      are reported and flagged with 🔄
    • shows up to *preview_n* example columns from head()
    """
    # ── headline ----------------------------------------------------------
    mem = df.memory_usage(deep=True).sum() / 1_048_576  # → MiB
    na_pct = df.isna().any(axis=1).mean() * 100
    num, obj, dt = (df.select_dtypes(t).shape[1]
                    for t in ["number", "object", "datetime"])

    print(f"   📄 {label:<6} | {df.shape[0]:,}×{df.shape[1]} "
          f"| mem:{mem:6.1f} MiB | num:{num} obj:{obj} dt:{dt} "
          f"| rows w/ NA≥1:{na_pct:6.2f}%")

    # ── diff vs. baseline -------------------------------------------------
    if baseline is not None:
        changed_cols = []
        renamed_cnt = 0
        dtype_cnt = 0

        for c in df.columns:
            if c not in baseline.columns:
                changed_cols.append(c)
                renamed_cnt += 1  # entirely new column
            else:
                if baseline[c].dtype != df[c].dtype:
                    changed_cols.append(c)
                    dtype_cnt += 1  # dtype fixed/changed

        if changed_cols:
            print(f"      ↪ 🔄 {len(changed_cols)} columns adjusted "
                  f"(renamed:{renamed_cnt}, dtype-fixed:{dtype_cnt})")
    else:
        changed_cols = list(df.columns)

    # choose columns to preview
    cols = changed_cols[:preview_n]
    if len(changed_cols) > preview_n:
        cols.append("…")  # visual ellipsis

    safe_cols = [c for c in cols if c in df.columns]  # drop the ellipsis

    # ── pretty head() -----------------------------------------------------
    with pd.option_context("display.max_colwidth", 60,
                           "display.width", 120):
        print(df[safe_cols].head(head_n).to_string(index=False))
    print("-" * 60)

"""## BEFORE vs AFTER visuals"""

def tweet_cleaning_plots(raw: pd.DataFrame, clean: pd.DataFrame):
    banner("✨  TWEET CLEANING — visual check")
    raw_len = raw["fulltext"].astype(str).str.len()
    clean_len = clean["clean_text"].str.len()
    pct_blank_raw = (raw["fulltext"].astype(str).str.strip() == "").mean() * 100
    pct_blank_clean = (clean["clean_text"].str.strip() == "").mean() * 100

    fig, ax = plt.subplots(1, 2, figsize=(10, 3.5))
    fig.suptitle("Tweet cleaning improvements", weight="bold")

    bins = np.linspace(0, max(raw_len.max(), clean_len.max()), 50)
    ax[0].hist(raw_len, bins=bins, alpha=.55, label="raw", color="#d55e00")
    ax[0].hist(clean_len, bins=bins, alpha=.55, label="clean", color="#0072b2")
    ax[0].set_title("Tweet length distribution")
    ax[0].set_xlabel("# characters");
    ax[0].set_ylabel("Count")
    ax[0].legend(frameon=False)

    bars = ax[1].bar(["raw", "clean"], [pct_blank_raw, pct_blank_clean], color=["#d55e00", "#0072b2"], alpha=.8)
    ax[1].set_title("% blank / missing tweets")
    ax[1].set_ylim(0, max(pct_blank_raw, pct_blank_clean) * 1.2 + 0.5)
    ax[1].set_ylabel("% of total")
    for bar in bars:
        ax[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + .1,
                   f"{bar.get_height():.2f}%", ha="center", va="bottom")
    plt.tight_layout();
    plt.show()


def price_cleaning_plots(raw: pd.DataFrame, clean: pd.DataFrame, sample_days: int = 7):
    banner("✨  PRICE CLEANING — visual check")
    raw = raw.copy()
    clean = clean.copy()
    raw["datetime"] = pd.to_datetime(raw["datetime"], utc=True).dt.tz_localize(None)
    if clean.index.tz is not None:
        clean = clean.tz_convert(None)

    cnt_raw = (raw.set_index("datetime")["close"].groupby(pd.Grouper(freq="D")).size())
    cnt_clean = (clean["close"].groupby(pd.Grouper(freq="D")).size())
    max_rows = max(cnt_raw.max(), cnt_clean.max())
    pct_raw, pct_clean = cnt_raw / max_rows * 100, cnt_clean / max_rows * 100

    fig, ax = plt.subplots(1, 2, figsize=(12, 3.5))
    ax[0].plot(pct_raw, label="raw", color="#d55e00", alpha=.7)
    ax[0].plot(pct_clean, label="clean", color="#0072b2", alpha=.9)
    ax[0].set_title("Daily data availability (%)")
    ax[0].set_ylabel("% of max rows");
    ax[0].legend(frameon=False)

    start = max(raw["datetime"].min(), clean.index.min()) + pd.Timedelta(days=3)
    end = start + pd.Timedelta(days=sample_days)
    raw_slice = raw[(raw["datetime"].between(start, end))]
    clean_slice = clean.loc[start:end]

    ax[1].plot(raw_slice.set_index("datetime")["close"],
               label="raw", color="#d55e00", alpha=.6,
               marker="o", linestyle="")
    ax[1].plot(clean_slice["close"], label="clean", color="#0072b2", alpha=.9, lw=2)
    ax[1].set_title(f"Close price (native sampling, {sample_days} days)")
    ax[1].legend(frameon=False)
    plt.tight_layout();
    plt.show()

"""## Data-loading & cleaning"""

def load_and_clean(path: str,
                   save_name: str,
                   *,
                   explicit_aliases: list[str] | None = None,
                   rename_to: str = "created_at",
                   preview_n: int = 6) -> pd.DataFrame:
    """Read CSV → tidy headers → timestamp coercion + raw / clean snapshots."""
    banner(f"📥  Loading {os.path.basename(path)}")

    # RAW snapshot ------------------------------------------------------
    raw = pd.read_csv(path, on_bad_lines="skip")
    raw_baseline = raw.copy(deep=False)  # for diff highlight
    snapshot_df(raw_baseline, "RAW", preview_n=preview_n)

    # header clean-up ---------------------------------------------------
    raw.columns = (raw.columns.str.strip()
                   .str.lower()
                   .str.replace(r"\s+", "_", regex=True))

    # timestamp detection ----------------------------------------------
    ts_col = next((c for c in (explicit_aliases or []) if c in raw.columns), None)
    if ts_col is None:
        for c in raw.columns:
            try:
                pd.to_datetime(raw[c].head(preview_n), utc=True, errors="raise")
                ts_col = c;
                break
            except Exception:
                continue
    if ts_col is None:
        raise ValueError("❌ No timestamp-like column found!")

    # rename & coerce to UTC
    raw = raw.rename(columns={ts_col: rename_to})
    raw[rename_to] = pd.to_datetime(raw[rename_to], utc=True, errors="coerce")

    # CLEAN snapshot ----------------------------------------------------
    snapshot_df(raw, "CLEAN", baseline=raw_baseline, preview_n=preview_n)

    # persist tidy copy -------------------------------------------------
    out_path = f"{CLEAN_DIR}/{save_name}"
    raw.to_csv(out_path, index=False)
    print(f"   ✔ Saved tidy copy → {out_path}")
    return raw

"""## NLP"""

def clean_text(raw: str) -> str:
    """URL-, mention-, hashtag- and punctuation-stripping + lowercase."""
    raw = re.sub(r"^rt @\w+:?", "", str(raw), flags=re.I)  # kill "RT @user:"
    raw = re.sub(r"https?://\S+", "", raw)  # URLs
    raw = re.sub(r"@[\w_]+", "", raw)  # mentions
    raw = re.sub(r"#[A-Za-z0-9_]+", "", raw)  # hashtags
    raw = re.sub(r"[^\w\s$]", " ", raw)  # punct
    return re.sub(r"\s{2,}", " ", raw).strip().lower()


def add_sentiment(tw: pd.DataFrame, *, stage: str = "initial") -> pd.DataFrame:
    """
    Clean tweets + VADER sentiment + keyword dummies.

    Parameters
    ----------
    stage : str
        Free-form tag inserted in the banner so you can see
        *which* NLP pass is running (e.g. "initial", "enriched").
    """
    banner(f"📝  NLP pass ({stage}) — sentiment & keywords")

    text_aliases = ["text", "full_text", "fulltext", "tweet",
                    "content", "body"]
    txt_col = next((c for c in text_aliases if c in tw.columns), None)
    if txt_col is None:
        raise ValueError("No tweet-text column found.")

    tw = tw.rename(columns={txt_col: "text"}).copy()
    tw["clean_text"] = tw["text"].apply(clean_text)

    # drop blanks
    before = len(tw)
    tw = tw[tw["clean_text"].str.len() > 0].reset_index(drop=True)
    print(f"   ↪ removed {before - len(tw):,} empty / URL-only tweets")

    # sentiment & length
    nltk.download("vader_lexicon", quiet=True)
    sia = SentimentIntensityAnalyzer()
    tw["sentiment"] = tw["clean_text"].map(lambda t: sia.polarity_scores(t)["compound"])
    tw["text_len"] = tw["clean_text"].str.len()

    # keyword dummies
    for kw in KEYWORDS:
        tw[f"kw_{kw.replace(' ', '_')}"] = tw["clean_text"].str.contains(kw, regex=False).astype(int)

    print(f"   ↪ added sentiment + {len(KEYWORDS)} keyword flags")
    return tw

"""## Prices & merge"""

def prep_prices(px: pd.DataFrame) -> pd.DataFrame:
    """Forward-fill minute bars; add forward returns (leakage-safe)."""
    banner("💰  Preparing price panel")

    # 1) parse & purge phantom rows
    px["datetime"] = pd.to_datetime(px["datetime"], utc=True, errors="coerce")
    px = px.dropna(subset=["datetime"]).copy()  # <-- kills NaT header rows

    # 2) numeric cols
    for col in ["close", "open", "high", "low", "volume"]:
        if col in px.columns:
            px[col] = pd.to_numeric(px[col], errors="coerce")

    px = px[["datetime", "close"]].dropna().sort_values("datetime")
    px["datetime"] = px["datetime"].dt.tz_localize(None)
    px.index = px["datetime"].dt.floor("T")

    full_idx = pd.date_range(px.index.min(), px.index.max(), freq="T")
    px = px.reindex(full_idx)
    px["close"].ffill(limit=4, inplace=True)

    px["ret_1m"] = px["close"].pct_change()
    for w in RET_WINDOWS:
        px[f"ret_fwd_{w}m"] = px["close"].pct_change(periods=w).shift(-w)
    return px.rename_axis("datetime")


def merge_tweets_prices(tweets: pd.DataFrame, prices: pd.DataFrame) -> pd.DataFrame:
    """
    Align each tweet to the minute‐bar TSLA price,
    and bring back a 'datetime' column for modeling.
    """
    tw = tweets.copy()
    # floor to minute, drop tz
    tw['ts_min'] = (
        pd.to_datetime(tw['created_at'], utc=True)
          .dt.tz_localize(None)
          .dt.floor('T')
    )
    # pick up close + your forward‐returns
    ret_cols = ['close'] + [f"ret_fwd_{w}m" for w in RET_WINDOWS]
    merged = pd.merge(
        tw,
        prices[ret_cols],
        left_on='ts_min',
        right_index=True,
        how='inner'
    )
    # create a proper datetime column
    merged['datetime'] = merged['ts_min']
    return merged

"""## Models:

### Baseline Model — Linear Ridge
"""

def run_ridge(df: pd.DataFrame, horizon=60, alpha_grid=[1e-4, 1e-3, 1e-2, .1, 1, 10, 100]):
    banner("📊  Baseline Model — Ridge (+CV)")
    y = df[f"ret_fwd_{horizon}m"].values
    X = df[["sentiment", *[f"kw_{kw.replace(' ', '_')}" for kw in KEYWORDS]]].values
    split = int(.7 * len(df))
    X_tr, X_te, y_tr, y_te = X[:split], X[split:], y[:split], y[split:]

    pipe = Pipeline([("scale", StandardScaler(with_mean=False)),
                     ("ridge", Ridge(random_state=RANDOM_STATE))])
    cv = TimeSeriesSplit(n_splits=5)
    gscv = GridSearchCV(pipe, {"ridge__alpha": alpha_grid},
                        cv=cv, scoring="neg_mean_squared_error", n_jobs=-1)
    gscv.fit(X_tr, y_tr)

    best_a = gscv.best_params_["ridge__alpha"]
    preds = gscv.predict(X_te)
    rmse = mean_squared_error(y_te, preds) ** .5
    r2 = r2_score(y_te, preds)
    print(f"   Best α = {best_a:.4g} | RMSE = {rmse:.6f} | R² = {r2:.4f}")

    plt.figure(figsize=(4, 4))
    plt.scatter(y_te, preds, alpha=.3)
    plt.axline((0, 0), slope=1, color="k")
    plt.xlabel("Actual fwd return");
    plt.ylabel("Predicted")
    plt.title(f"Ridge (α={best_a})  — predicted vs. actual")
    plt.tight_layout();
    plt.show()
    return gscv.best_estimator_

"""###  Advanced Model — XGBoost

#### XGBoost helper
"""

def _select_top_tfidf(df: pd.DataFrame,
                      horizon: int = 60,
                      max_keep: int = 400) -> pd.DataFrame:
    """
    1–2-gram TF-IDF → keep *max_keep* terms with highest |r| to forward returns.
    Returns a DataFrame indexed like *df*.
    """
    from sklearn.feature_extraction.text import TfidfVectorizer

    # fit the TF-IDF
    tv = TfidfVectorizer(max_features=4_000,
                         ngram_range=(1, 2),
                         min_df=15)
    Xtf = tv.fit_transform(df["clean_text"])  # shape = (n_samples, n_terms)

    # compute Pearson r between each term-column and y
    y = df[f"ret_fwd_{horizon}m"].fillna(0).values
    terms = tv.get_feature_names_out()

    corrs = []
    for i in range(Xtf.shape[1]):
        col = Xtf[:, i].toarray().ravel()
        # guard against NaNs
        r = np.corrcoef(col, y)[0, 1]
        corrs.append(0.0 if np.isnan(r) else r)
    corrs = np.array(corrs)

    # pick the top |r|-features
    keep_idx = np.argsort(np.abs(corrs))[-max_keep:]
    Xsel = Xtf[:, keep_idx].toarray()
    sel_terms = terms[keep_idx]

    # return as DataFrame
    cols = [f"tfidf_{t}" for t in sel_terms]
    return pd.DataFrame(Xsel, columns=cols, index=df.index)


def _clock_features(timestamps: pd.Series) -> pd.DataFrame:
    """
    Cyclical encoding of time‐of‐day plus a rush‐hour flag.
    Accepts a pd.Series of Timestamps (e.g. df['datetime']).
    Returns a DataFrame with the same index.
    """
    ts = pd.to_datetime(timestamps, utc=True)  # ensure datetime64[ns, UTC]
    mins = ts.dt.hour * 60 + ts.dt.minute  # minutes since midnight
    radians = 2 * np.pi * mins / (24 * 60)  # full‐day cycle

    # rush hours: 09:00–10:00 or 15:00–16:00 (note: times in UTC if your tz is UTC)
    rush = (
            ((mins >= 9 * 60) & (mins < 10 * 60)) |
            ((mins >= 15 * 60) & (mins < 16 * 60))
    ).astype(int)

    return pd.DataFrame({
        "tod_sin": np.sin(radians),
        "tod_cos": np.cos(radians),
        "rush": rush
    }, index=timestamps.index)


def _interactions(base: pd.DataFrame,
                  rush_flag: pd.Series) -> pd.DataFrame:
    """
    Build simple interaction terms:
      • sentiment × rush
      • each kw_* dummy × rush  (logical AND → preserves binary 0/1)
    """
    out = {
        "sent_rush": base["sentiment"] * rush_flag
    }
    for col in base.columns:
        if col.startswith("kw_"):
            out[f"{col}_rush"] = (base[col].astype(bool) & rush_flag.astype(bool)).astype(int)
    return pd.DataFrame(out, index=base.index)

"""#### XGBoost Model"""

# --------------------------------------------------------------------- #
# A.  feature builder
# --------------------------------------------------------------------- #
def build_feature_matrix(
        df: pd.DataFrame,
        horizon:   int = 60,
        tfidf_max: int = 400
    ) -> Tuple[pd.DataFrame, pd.Series]:
    """
    Compose X, y:
      - uses df['datetime'] if present, otherwise the index
    """
    # target
    df = df.dropna(subset=[f"ret_fwd_{horizon}m"]).copy()
    df['target'] = (df[f"ret_fwd_{horizon}m"] > 0).astype(int)

    # base tweet block
    kw_cols = [f"kw_{kw.replace(' ', '_')}" for kw in KEYWORDS]
    tweet_blk = df[["sentiment", "text_len", *kw_cols]]

    blocks = [tweet_blk]

    # top‐tfidf block (as before) …
    from sklearn.feature_extraction.text import TfidfVectorizer
    tv = TfidfVectorizer(max_features=4000, ngram_range=(1, 2), min_df=15)
    Xtf = tv.fit_transform(df["clean_text"])
    y = df[f"ret_fwd_{horizon}m"].fillna(0).values
    # compute correlation & select
    corrs = np.nan_to_num([
        np.corrcoef(col.toarray().ravel(), y)[0, 1]
        for col in Xtf.T
    ])
    keep = np.argsort(np.abs(corrs))[-tfidf_max:]
    tf_sel = Xtf[:, keep].toarray()
    tf_cols = [f"tfidf_{t}" for t in np.array(tv.get_feature_names_out())[keep]]
    blocks.append(pd.DataFrame(tf_sel, columns=tf_cols, index=df.index))

    # clock features: always pass a Series of timestamps
    if "datetime" in df.columns:
        ts = df["datetime"]
    else:
        ts = pd.Series(df.index, index=df.index)
    clk = _clock_features(ts)
    blocks.append(clk)
    blocks.append(_interactions(tweet_blk, clk["rush"]))

    # engagement counts
    for c in ("likecount", "retweetcount", "quotecount", "replycount"):
        if c in df.columns:
            blocks.append(df[c].fillna(0).to_frame())

    X = pd.concat(blocks, axis=1)
    return X, df["target"]

# --------------------------------------------------------------------- #
# B.  trainer
# --------------------------------------------------------------------- #

try:
    # newer versions
    from xgboost.callback import EarlyStopping
except ImportError:
    EarlyStopping = None


# ── 7-C. XGBoost (robust to missing args) ───────────────────────────
def train_xgboost(df: pd.DataFrame,
                  horizon:     int   = 60,
                  *,
                  tfidf_max:   int   = 400,
                  prec_target: float = 0.50,
                  user_params: Dict[str, object]|None = None):
    banner("🚀  Advanced Model — XGBoost")

    # build
    X, y   = build_feature_matrix(df, horizon=horizon, tfidf_max=tfidf_max)
    split  = int(.7 * len(X))
    X_tr, X_te = X.iloc[:split], X.iloc[split:]
    y_tr, y_te = y.iloc[:split], y.iloc[split:]

    # params + class balance
    params = {
        **XGB_PARAMS,
        **(user_params or {}),
        "max_depth":        3,
        "min_child_weight": 20,
        "reg_lambda":       2.0,
        "gamma":            0.10,
        "scale_pos_weight": (y_tr == 0).sum() / max((y_tr == 1).sum(), 1),
    }

    model = XGBClassifier(**params)
    # robust early stopping
    fit_kwargs = dict(eval_set=[(X_tr, y_tr), (X_te, y_te)], verbose=False)
    try:
        model.fit(X_tr, y_tr, early_stopping_rounds=50, **fit_kwargs)
    except TypeError:
        model.fit(X_tr, y_tr, **fit_kwargs)

    # threshold search for best F1
    prob_te = model.predict_proba(X_te)[:, 1]
    prec, rec, thr = precision_recall_curve(y_te, prob_te)
    f1_scores      = 2 * prec * rec / (prec + rec + 1e-8)
    best_idx       = int(f1_scores.argmax())
    best_th        = thr[best_idx]

    print(
        f"\n★ Optimal threshold τ ≈ {best_th:.3f} "
        f"(F1 = {f1_scores[best_idx]:.3f} | P = {prec[best_idx]:.3f} | R = {rec[best_idx]:.3f})"
    )

    # final metrics
    y_pred = (prob_te >= best_th).astype(int)
    print(f"Accuracy  : {accuracy_score(y_te, y_pred):.3f}")
    print(f"Precision : {precision_score(y_te, y_pred):.3f}")
    print(f"Recall    : {recall_score(y_te, y_pred):.3f}")
    print(f"F1-score  : {f1_scores[best_idx]:.3f}")
    print(f"ROC AUC   : {roc_auc_score(y_te, prob_te):.3f}\n")

    # diagnostics…
    RocCurveDisplay.from_predictions(y_te, prob_te);        plt.title("ROC — XGB"); plt.show()
    ConfusionMatrixDisplay.from_predictions(y_te, y_pred, cmap="viridis"); plt.show()

    evals = model.evals_result()
    if evals:
        plt.figure(figsize=(6,3))
        plt.plot(evals["validation_0"]["logloss"], label="train")
        plt.plot(evals["validation_1"]["logloss"], label="valid")
        plt.legend(); plt.tight_layout(); plt.show()

    top15 = pd.Series(model.feature_importances_, index=X.columns).nlargest(15)
    top15.plot(kind="barh", figsize=(7,4), title="Top XGB features")
    plt.gca().invert_yaxis(); plt.tight_layout(); plt.show()

    return model

"""## Keyword booster"""

def top_correlated_terms(tweets: pd.DataFrame, n_terms=15) -> List[str]:
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer(max_features=500, ngram_range=(1, 2), min_df=20)
    X = tfidf.fit_transform(tweets["clean_text"])
    terms = np.array(tfidf.get_feature_names_out())
    y = tweets["ret_fwd_60m"].fillna(0).values
    corr = np.array([np.corrcoef(X[:, i].toarray().ravel(), y)[0, 1]
                     for i in range(X.shape[1])])
    return terms[np.argsort(np.abs(corr))[-n_terms:]].tolist()

"""## Main"""

def main() -> None:
    banner("🏁  PIPELINE START")

    # ── Tweets -----------------------------------------------------------
    tweets_raw = load_and_clean(
        MUSK_CSV,
        "clean_all_musk_posts.csv",
        explicit_aliases=["created_at", "timestamp", "date", "created", "createdat"],
        rename_to="created_at",
        preview_n=8  # ← show an 8×8 preview instead of the default 6×6
    )

    # ── Prices -----------------------------------------------------------
    prices_raw = load_and_clean(
        TSLA_CSV,
        "clean_tesla_prices.csv",
        explicit_aliases=["datetime", "timestamp", "price", "date"],
        rename_to="datetime",
        preview_n=8
    )

    tweets = add_sentiment(tweets_raw, stage="initial")
    prices = prep_prices(prices_raw)
    merged = merge_tweets_prices(tweets, prices)

    banner("🔍  Auto-keyword discovery (TF-IDF ↔ returns)")
    extra_kw = top_correlated_terms(merged, n_terms=15)
    if extra_kw:
        print("   ➕ Added:", extra_kw)
        KEYWORDS[:] = list(dict.fromkeys(KEYWORDS + extra_kw))
        tweets = add_sentiment(tweets_raw, stage="enriched")  # rebuild flags
        merged = merge_tweets_prices(tweets, prices)  # refresh merge

    tweet_cleaning_plots(tweets_raw, tweets)
    prices_raw["close"] = pd.to_numeric(prices_raw["close"], errors="coerce")
    price_cleaning_plots(prices_raw, prices)

    run_ridge(merged, horizon=60)
    train_xgboost(merged, horizon=60)

    banner("✅  PIPELINE FINISHED")

# ── Entrypoint ────────────────────────────────────────
if __name__ == "__main__":
    main()