# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2Tg8jBhDb7hfbupcQFxD8sPBs1jzyEN
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
elon_tsla_eda.py
================
EDA + cleaning for:
    “Elon Musk tweets → TSLA intraday impact”

Outputs (in …/clean Data/):
    • clean_all_musk_posts.csv
    • clean_tesla_prices.csv
    • clean_tesla_prices_panel.csv
    • merged_tweets_prices.csv

Run BEFORE elon_tsla_models.py
"""
# ───────────────────────── Imports ───────────────────────────────────
import os, re, warnings
from typing import List

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

warnings.filterwarnings("ignore")
plt.style.use("ggplot")
RANDOM_STATE = 42

# ───────────────────────── Drive paths ───────────────────────────────
try:
    from google.colab import drive
    drive.mount("/content/drive")
except ImportError:
    pass

BASE_DIR  = (
    "/content/drive/My Drive/Notability/Year 4/"
    "Second semester/Data Science/Final Project/Data"
)
MUSK_CSV  = f"{BASE_DIR}/all_musk_posts.csv"
TSLA_CSV  = f"{BASE_DIR}/tesla_stock_data_2000_2025.csv"
CLEAN_DIR = f"{BASE_DIR}/clean Data"          # ← unified lowercase
os.makedirs(CLEAN_DIR, exist_ok=True)

# ─────────────────────── Constants ──────────────────────────────────
KEYWORDS: List[str] = [
    "model 3", "model y", "cybertruck", "ai",
    "robot", "teslabot", "fsd", "tesla energy", "spacex",
]
RET_WINDOWS = [5, 30, 60]

# ─────────────────────── Console helpers ────────────────────────────
def banner(txt): print(f"\n{'─'*60}\n{txt}\n{'─'*60}")

def snapshot_df(df: pd.DataFrame, label: str,
                *, head_n=4, preview_n=6,
                baseline: pd.DataFrame | None = None) -> None:
    mem = df.memory_usage(deep=True).sum()/1_048_576
    na  = df.isna().any(axis=1).mean()*100
    num,obj,dt = (df.select_dtypes(t).shape[1]
                  for t in ["number","object","datetime64[ns]"])
    print(f"   📄 {label:<6} | {df.shape[0]:,}×{df.shape[1]} "
          f"| mem:{mem:6.1f} MiB | num:{num} obj:{obj} dt:{dt} "
          f"| rows w/ NA≥1:{na:6.2f}%")
    changed = (df.columns if baseline is None else
               [c for c in df.columns
                if c not in baseline.columns
                or df[c].dtype != baseline[c].dtype])
    if baseline is not None and changed:
        print(f"      ↪ 🔄 {len(changed)} columns adjusted")
    cols = list(changed[:preview_n]) + (["…"] if len(changed)>preview_n else [])
    with pd.option_context("display.max_colwidth",60,"display.width",120):
        print(df[[c for c in cols if c in df.columns]]
              .head(head_n).to_string(index=False))
    print("-"*60)

# ─────────────────── CSV loader / header fixer ──────────────────────
def load_and_clean(path, save_name,
                   *, explicit_aliases=None,
                   rename_to="created_at",
                   preview_n=6):
    banner(f"📥  Loading {os.path.basename(path)}")
    raw = pd.read_csv(path, on_bad_lines="skip")
    snapshot_df(raw.copy(), "RAW", preview_n=preview_n)

    raw.columns = (raw.columns.str.strip()
                             .str.lower()
                             .str.replace(r"\s+","_",regex=True))

    ts_col = next((c for c in (explicit_aliases or []) if c in raw.columns), None)
    if ts_col is None:
        for c in raw.columns:
            try:
                pd.to_datetime(raw[c].head(preview_n), utc=True, errors="raise")
                ts_col=c; break
            except Exception: pass
    if ts_col is None:
        raise ValueError("❌ No timestamp-like column found.")

    raw = raw.rename(columns={ts_col:rename_to})
    raw[rename_to] = pd.to_datetime(raw[rename_to], utc=True, errors="coerce")

    snapshot_df(raw, "CLEAN", preview_n=preview_n)
    out = f"{CLEAN_DIR}/{save_name}"
    raw.to_csv(out, index=False)
    print(f"   ✔ Saved tidy copy → {out}")
    return raw

# ───────────────────── Tweet NLP & sentiment ────────────────────────
def clean_text(t:str)->str:
    t=re.sub(r"^rt @\w+:?","",str(t),flags=re.I)
    t=re.sub(r"https?://\S+","",t)
    t=re.sub(r"@[\w_]+","",t)
    t=re.sub(r"#[A-Za-z0-9_]+","",t)
    t=re.sub(r"[^\w\s$]"," ",t)
    return re.sub(r"\s{2,}"," ",t).strip().lower()

def add_sentiment(df: pd.DataFrame) -> pd.DataFrame:
    banner("📝  NLP pass — sentiment & keywords")
    txt_col = next((c for c in
                    ["text","full_text","fulltext","tweet","content","body"]
                    if c in df.columns), None)
    tw = df.rename(columns={txt_col:"text"}).copy()

    tw["clean_text"] = tw["text"].apply(clean_text)
    pct_drop = (tw["clean_text"].str.len()==0).mean()*100
    tw = tw[tw["clean_text"].str.len()>0].reset_index(drop=True)
    print(f"   ↪ removed {pct_drop:.2f}% of tweets that became blank")

    nltk.download("vader_lexicon", quiet=True)
    sia = SentimentIntensityAnalyzer()
    tw["sentiment"] = tw["clean_text"].map(lambda t:sia.polarity_scores(t)["compound"])
    tw["text_len"]  = tw["clean_text"].str.len()

    for kw in KEYWORDS:
        tw[f"kw_{kw.replace(' ','_')}"] = tw["clean_text"].str.contains(kw,regex=False).astype(int)
    print(f"   ↪ added sentiment + {len(KEYWORDS)} keyword flags")

    tw.attrs["pct_dropped"] = pct_drop
    return tw

# ────────────────── Minute-bar panel + forward returns ───────────────
def prep_prices(px: pd.DataFrame) -> pd.DataFrame:
    banner("💰  Preparing price panel")
    px["datetime"]=pd.to_datetime(px["datetime"],utc=True,errors="coerce")
    px=px.dropna(subset=["datetime"]).copy()
    for c in ["close","open","high","low","volume"]:
        if c in px.columns: px[c]=pd.to_numeric(px[c],errors="coerce")

    px=px[["datetime","close"]].dropna().sort_values("datetime")
    px["datetime"]=px["datetime"].dt.tz_localize(None)
    px.index=px["datetime"].dt.floor("T")

    full_idx=pd.date_range(px.index.min(),px.index.max(),freq="T")
    px=px.reindex(full_idx)
    px["close"].ffill(limit=4,inplace=True)

    px["ret_1m"]=px["close"].pct_change()
    for w in RET_WINDOWS:
        px[f"ret_fwd_{w}m"]=px["close"].pct_change(periods=w).shift(-w)
    return px.rename_axis("datetime")

# ───────────────────────── Visuals ───────────────────────────────────
def tweet_cleaning_plots(raw, clean):
    banner("✨  TWEET CLEANING — visual check")
    pct_before = clean.attrs["pct_dropped"]
    raw_len, clean_len = raw["fulltext"].astype(str).str.len(), clean["clean_text"].str.len()

    fig,ax=plt.subplots(1,2,figsize=(10,3.5))
    fig.suptitle("Tweet cleaning improvements",weight="bold")
    bins=np.linspace(0,max(raw_len.max(),clean_len.max()),50)
    ax[0].hist(raw_len,bins=bins,alpha=.55,label="raw",color="#d55e00")
    ax[0].hist(clean_len,bins=bins,alpha=.55,label="clean",color="#0072b2")
    ax[0].set_title("Tweet length distribution")
    ax[0].set_xlabel("# characters"); ax[0].set_ylabel("Count"); ax[0].legend(frameon=False)

    bars=ax[1].bar(["before","after"],[pct_before,0],color=["#d55e00","#0072b2"],alpha=.8)
    ax[1].set_title("% dropped (blank after cleaning)")
    ax[1].set_ylim(0,pct_before*1.2+0.5); ax[1].set_ylabel("% of total")
    for bar in bars:
        ax[1].text(bar.get_x()+bar.get_width()/2,bar.get_height()+.1,
                   f"{bar.get_height():.2f}%",ha="center",va="bottom")
    plt.tight_layout(); plt.show()

def price_cleaning_plots(raw, clean, sample_days=7):
    banner("✨  PRICE CLEANING — visual check")
    raw = raw.copy()
    raw["datetime"] = pd.to_datetime(raw["datetime"], utc=True).dt.tz_localize(None)
    if clean.index.tz is not None:
        clean = clean.tz_convert(None)

    # rolling share
    days = pd.date_range(raw["datetime"].min().date(),
                         raw["datetime"].max().date(), freq="D")
    raw_days   = (raw.set_index("datetime").assign(present=1)["present"]
                    .resample("D").max().reindex(days).fillna(0))
    clean_days = (clean.assign(present=1)["present"]
                    .resample("D").max().reindex(days).fillna(0))

    fig, ax = plt.subplots(1,2,figsize=(14,4))
    ax[0].plot(raw_days.rolling(30).mean()*100,color="#d55e00",lw=1.1)
    ax[0].fill_between(raw_days.index,raw_days.rolling(30).mean()*100,alpha=.15,color="#d55e00")
    ax[0].plot(clean_days.rolling(30).mean()*100,color="#0072b2",lw=1.3)
    ax[0].set_title("30-day rolling share of trading days with data")
    ax[0].set_ylabel("% of days"); ax[0].set_ylim(0,105)
    ax[0].grid(alpha=.3,ls="--",lw=.5)
    ax[0].legend(["raw (daily)","clean (minute)"],frameon=False)

    # 7-day window (auto-expand until ≥3 pts each)
    uniq_days = np.unique(raw["datetime"].dt.normalize())
    start_idx = 3 if len(uniq_days)>3 else 0
    for extra in range(0,30):    # expand up to 30 days if needed
        start = uniq_days[start_idx]
        end   = start + pd.Timedelta(days=sample_days-1+extra)
        r = (raw[(raw["datetime"].between(start,end))]
             .set_index("datetime")["close"]
             .resample("D").last().dropna())
        c = (clean.loc[start:end]["close"]
             .resample("D").last().dropna())
        if len(r)>=3 and len(c)>=3:
            raw_slice, clean_slice = r, c
            break
    else:
        raw_slice = clean_slice = pd.Series(dtype=float)

    ax[1].plot(clean_slice.index,clean_slice.values,lw=2,color="#0072b2",label="clean")
    ax[1].scatter(raw_slice.index,raw_slice.values,s=45,color="#d55e00",label="raw",zorder=3)
    ax[1].set_title(f"Daily close price (~{len(clean_slice)} days window)")
    ax[1].grid(alpha=.3,ls="--",lw=.5)
    ax[1].tick_params(axis="x",rotation=45)
    if not raw_slice.empty or not clean_slice.empty:
        ax[1].legend(frameon=False)
    plt.tight_layout(); plt.show()

# ───────────── merge tweets ↔ prices & save --------------------------
def merge_tweets_prices(tw, px):
    tw = tw.copy()
    tw["ts_min"] = (pd.to_datetime(tw["created_at"],utc=True)
                      .dt.tz_localize(None).dt.floor("T"))
    ret_cols = ["close"] + [f"ret_fwd_{w}m" for w in RET_WINDOWS]
    return pd.merge(tw, px[ret_cols], left_on="ts_min", right_index=True, how="inner")

# ───────────────────────────── Main ─────────────────────────────────
def main():
    banner("🏁  EDA PIPELINE START")

    tweets_raw = load_and_clean(
        MUSK_CSV,"clean_all_musk_posts.csv",
        explicit_aliases=["created_at","timestamp","date","created"],
        rename_to="created_at",preview_n=8)
    tweets = add_sentiment(tweets_raw)
    tweet_cleaning_plots(tweets_raw,tweets)
    tweets.to_csv(f"{CLEAN_DIR}/clean_all_musk_posts.csv",index=False)

    prices_raw = load_and_clean(
        TSLA_CSV,"clean_tesla_prices.csv",
        explicit_aliases=["datetime","timestamp","price","date"],
        rename_to="datetime",preview_n=8)
    prices = prep_prices(prices_raw)
    price_cleaning_plots(prices_raw,prices)
    prices.to_csv(f"{CLEAN_DIR}/clean_tesla_prices_panel.csv")

    merge_tweets_prices(tweets,prices).to_csv(f"{CLEAN_DIR}/merged_tweets_prices.csv",index=False)

    banner("✅  EDA PIPELINE FINISHED")

# ─────────────────────── Entrypoint ─────────────────────────────────
if __name__ == "__main__":
    main()